---
title: Calibration, Net Re-Classification Index and Goodness-of-Fit Test for Logistic Regression
author: null
date: '2019-09-05'
slug: calibration-NRI-goodness-of-fit-test-for-logistic-regression
categories: [R, regression, prediction]
tags: [goodness-of-fit, calibration]
editor_options: 
  chunk_output_type: console
---



<p>This post shows how to calculate the Net Re-Classification Index and Goodness-of-Fit Test (Hosmer-Lemeshow Test) for logistic regression models. In addition, we show how to plot the calibration curves.</p>
<pre class="r"><code>#  load packages ----------------------------------------------------------

if (!requireNamespace(&quot;pacman&quot;, quietly = TRUE)) {
  install.packages(&quot;pacman&quot;)
}
pacman::p_load(PredictABEL)
pacman::p_load(aod)
pacman::p_load(ggplot2)

# packages for ggplot2 themes
pacman::p_load_gh(&quot;hrbrmstr/hrbrthemes&quot;)
pacman::p_load(ggrepel)
pacman::p_load(Cairo)</code></pre>
<pre><code>## Installing package into &#39;/home/sahir/R/x86_64-pc-linux-gnu-library/3.6&#39;
## (as &#39;lib&#39; is unspecified)</code></pre>
<pre><code>## Warning in utils::install.packages(package, ...): installation of package
## &#39;Cairo&#39; had non-zero exit status</code></pre>
<pre><code>## Warning in p_install(package, character.only = TRUE, ...):</code></pre>
<pre><code>## Warning in library(package, lib.loc = lib.loc, character.only = TRUE,
## logical.return = TRUE, : there is no package called &#39;Cairo&#39;</code></pre>
<pre><code>## Warning in pacman::p_load(Cairo): Failed to install/load:
## Cairo</code></pre>
<pre class="r"><code>pacman::p_load(extrafont)
extrafont::loadfonts()




# colors and themes -------------------------------------------------------

# color blind palette
cbbPalette &lt;- c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;)

# my theme defaults
gg_sy &lt;- theme(legend.position = &quot;bottom&quot;, 
               axis.text = element_text(size = 20),
               axis.title = element_text(size = 20), 
               legend.text = element_text(size = 20), 
               legend.title = element_text(size = 20))




#  load data --------------------------------------------------------------


mydata &lt;- read.csv(&quot;https://stats.idre.ucla.edu/stat/data/binary.csv&quot;)
## view the first few rows of the data
head(mydata)</code></pre>
<pre><code>##   admit gre  gpa rank
## 1     0 380 3.61    3
## 2     1 660 3.67    3
## 3     1 800 4.00    1
## 4     1 640 3.19    4
## 5     0 520 2.93    4
## 6     1 760 3.00    2</code></pre>
<pre class="r"><code>mydata$rank &lt;- factor(mydata$rank)
mydata$admitf &lt;- factor(mydata$admit, levels = c(0,1), labels = c(&quot;no admit&quot;, &quot;admit&quot;))

# without rank
m1 &lt;- glm(admit ~ gre + gpa, data = mydata, family = &quot;binomial&quot;)
preds_m1 &lt;- predict(m1, type = &quot;response&quot;) # predicted probs from model 1

# with rank
m2 &lt;- glm(admit ~ gre + gpa + rank, data = mydata, family = &quot;binomial&quot;)
preds_m2 &lt;- predict(m2, type = &quot;response&quot;) # predicted probs from model 2



# Calibration plots and Hosmer Lemeshow test ------------------------------


# In this example, we see that the $Chi_square value is smaller for m2, compared to m1
# The null hypothesis is that we have a good fit. So the larger the p-value, the smaller the 
# Chi_square statistic, the better the fit. So we can say that rank has added value to the model

# Calibration plot for m1
PredictABEL::plotCalibration(data = as.matrix(mydata$admit), # observed response
                             cOutcome = 1, # the category corresponding to outcome of interest
                             predRisk = preds_m1, # predicted probs
                             groups = 10 # number of groups to bin the predicted probabilities into, usually 10 is default
                             )</code></pre>
<pre><code>## $Table_HLtest
##                total meanpred meanobs predicted observed
## [0.0978,0.184)    40    0.156   0.150      6.25        6
## [0.1839,0.224)    40    0.202   0.125      8.09        5
## [0.2241,0.254)    40    0.239   0.225      9.56        9
## [0.2536,0.289)    40    0.272   0.275     10.89       11
## [0.2894,0.311)    40    0.299   0.375     11.97       15
## [0.3110,0.341)    40    0.326   0.375     13.04       15
## [0.3406,0.370)    40    0.354   0.375     14.16       15
## [0.3702,0.408)    40    0.391   0.450     15.64       18
## [0.4079,0.461)    40    0.431   0.375     17.26       15
## [0.4614,0.555]    40    0.503   0.450     20.13       18
## 
## $Chi_square
## [1] 4.706
## 
## $df
## [1] 8
## 
## $p_value
## [1] 0.7885</code></pre>
<pre class="r"><code># Calibration plot for m2
PredictABEL::plotCalibration(data = as.matrix(mydata$admit), # observed response
                             cOutcome = 1, # the category corresponding to outcome of interest
                             predRisk = preds_m1, # predicted probs
                             groups = 10 # number of groups to bin the predicted probabilities into, usually 10 is default, so you will see 10 categories and 10 points on the calibration plot
)</code></pre>
<p><img src="/blog/post/2019-09-05-calibration-net-re-classification-and-goodness-of-fit-test-for-logistic-regression_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre><code>## $Table_HLtest
##                total meanpred meanobs predicted observed
## [0.0978,0.184)    40    0.156   0.150      6.25        6
## [0.1839,0.224)    40    0.202   0.125      8.09        5
## [0.2241,0.254)    40    0.239   0.225      9.56        9
## [0.2536,0.289)    40    0.272   0.275     10.89       11
## [0.2894,0.311)    40    0.299   0.375     11.97       15
## [0.3110,0.341)    40    0.326   0.375     13.04       15
## [0.3406,0.370)    40    0.354   0.375     14.16       15
## [0.3702,0.408)    40    0.391   0.450     15.64       18
## [0.4079,0.461)    40    0.431   0.375     17.26       15
## [0.4614,0.555]    40    0.503   0.450     20.13       18
## 
## $Chi_square
## [1] 4.706
## 
## $df
## [1] 8
## 
## $p_value
## [1] 0.7885</code></pre>
<pre class="r"><code># Boxplots for discrimination ---------------------------------------------

df1 &lt;- data.frame(admit = mydata$admitf, 
                  model = &quot;w/o rank&quot;, 
                  predicted = preds_m1,
                  stringsAsFactors = FALSE)

df2 &lt;- data.frame(admit = mydata$admitf, 
                  model = &quot;with rank&quot;, 
                  predicted = preds_m2,
                  stringsAsFactors = FALSE)
dfplot &lt;- rbind(df1, df2) 
dfplot$model &lt;- factor(dfplot$model)

ggplot(dfplot, aes(x = admit, y = predicted, fill = model)) +
  geom_boxplot() +
  scale_fill_manual(values=cbbPalette[c(3,7)], guide=guide_legend(ncol = 2)) +
  labs(x=&quot;&quot;, y=&quot;Predicted probability&quot;,
       title=&quot;Predicted probability by observed outcome for each model&quot;,
       subtitle=&quot;Based on logistic regression model adjusted for GRE and GPA&quot;,
       caption=&quot;&quot;) +
  theme_ipsum_rc() + 
  theme(legend.position = &quot;bottom&quot;, 
        axis.text = element_text(size = 20),
        axis.title = element_text(size = 20), 
        legend.text = element_text(size = 20), 
        legend.title = element_text(size = 20))</code></pre>
<p><img src="/blog/post/2019-09-05-calibration-net-re-classification-and-goodness-of-fit-test-for-logistic-regression_files/figure-html/unnamed-chunk-1-2.png" width="672" /></p>
<pre class="r"><code># Net reclassification index ----------------------------------------------

#&#39; Re-classfication tables quantify the model improvement of one model over
#&#39; another by counting the number of individuals who move to another risk
#&#39; category or remain in the same risk category as a result of updating the risk
#&#39; model. Any _upward_ movement in categories for event subjects (i.e. those
#&#39; with admit=1) implies improved classification, and any _downward_ movement
#&#39; indicates worse reclassification. The interpretation is opposite for people
#&#39; who do not develop events (admit = 0). Smaller p-values indicate better
#&#39; reclassification accuracy.
#&#39; 

PredictABEL::reclassification(data = as.matrix(mydata$admit), 
                              cOutcome = 1, 
                              predrisk1 = preds_m1, # original model
                              predrisk2 = preds_m2, # updated model
                              cutoff = seq(0, 1, by = 0.20))</code></pre>
<pre><code>##  _________________________________________
##  
##      Reclassification table    
##  _________________________________________
## 
##  Outcome: absent 
##   
##              Updated Model
## Initial Model [0,0.2) [0.2,0.4) [0.4,0.6) [0.6,0.8) [0.8,1]  % reclassified
##     [0,0.2)        31        16         0         0       0              34
##     [0.2,0.4)      52        99        24         0       0              43
##     [0.4,0.6)       0        26        18         7       0              65
##     [0.6,0.8)       0         0         0         0       0             NaN
##     [0.8,1]         0         0         0         0       0             NaN
## 
##  
##  Outcome: present 
##   
##              Updated Model
## Initial Model [0,0.2) [0.2,0.4) [0.4,0.6) [0.6,0.8) [0.8,1]  % reclassified
##     [0,0.2)         7         2         0         0       0              22
##     [0.2,0.4)      11        40        28         0       0              49
##     [0.4,0.6)       0        11        15        13       0              62
##     [0.6,0.8)       0         0         0         0       0             NaN
##     [0.8,1]         0         0         0         0       0             NaN
## 
##  
##  Combined Data 
##   
##              Updated Model
## Initial Model [0,0.2) [0.2,0.4) [0.4,0.6) [0.6,0.8) [0.8,1]  % reclassified
##     [0,0.2)        38        18         0         0       0              32
##     [0.2,0.4)      63       139        52         0       0              45
##     [0.4,0.6)       0        37        33        20       0              63
##     [0.6,0.8)       0         0         0         0       0             NaN
##     [0.8,1]         0         0         0         0       0             NaN
##  _________________________________________
## 
##  NRI(Categorical) [95% CI]: 0.2789 [ 0.1343 - 0.4235 ] ; p-value: 0.00016 
##  NRI(Continuous) [95% CI]: 0.4543 [ 0.2541 - 0.6545 ] ; p-value: 1e-05 
##  IDI [95% CI]: 0.0546 [ 0.0309 - 0.0783 ] ; p-value: 1e-05</code></pre>
<div id="further-reading-updated-january-10-2020" class="section level1">
<h1>Further Reading (UPDATED January 10, 2020)</h1>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
Good to raise aware of calibration but Hosmer-Lemeshow test doesn't assess calibration (and has many problems, e.g., summarised in Box G of <a href="https://t.co/4yiJe4bDu3">https://t.co/4yiJe4bDu3</a>), plus there a number of problems with the NRI (e.g., <a href="https://t.co/TkukdbV56l">https://t.co/TkukdbV56l</a>, <a href="https://t.co/EYyfx55BQT">https://t.co/EYyfx55BQT</a>)
</p>
— Gary Collins 🇪🇺 (<span class="citation">@GSCollins</span>) <a href="https://twitter.com/GSCollins/status/1169982080261996545?ref_src=twsrc%5Etfw">September 6, 2019</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
